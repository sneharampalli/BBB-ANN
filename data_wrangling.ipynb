{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All import statements\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import json\n",
    "\n",
    "# Number of iterations set to 1000\n",
    "d1 = 10\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of sigmoid function\n",
    "def deriv_sigmoid(x):\n",
    "    return sigmoid(x) - np.square(sigmoid(x))\n",
    "\n",
    "# Relu function\n",
    "def relu(x):\n",
    "    return np.max(0, x)        \n",
    "\n",
    "# Derivative of relu function\n",
    "def deriv_relu(x):\n",
    "    return np.sign(x)\n",
    "\n",
    "# Cleaning up the data\n",
    "def single_layer_perceptron(X_train, y_train, step_size, iterations, W_1, W_2, b_1, b_2, func_type):\n",
    "    # Stochastic Gradient Descent\n",
    "    for x in range(iterations):\n",
    "        z_1 = W_1 * X_train.T + b_1\n",
    "        if func_type is 'sigmoid':\n",
    "            a_1 = sigmoid(z_1)\n",
    "            g_der = deriv_sigmoid(z_1)\n",
    "        else: \n",
    "            a_1 = relu(z_1)\n",
    "            g_der = deriv_relu_inter(z_1)\n",
    "            g_der[g_der < 0] = 0\n",
    "        f_wb = W_2 * a_1 + b_2\n",
    "        g = sigmoid(f_wb)  \n",
    "\n",
    "        deriv_W1 = np.multiply((W_2.T * (g - y_train.T)), g_der) * X_train / m;\n",
    "        deriv_W2 = (g - y_train.T) * a_1.T / m\n",
    "\n",
    "        deriv_b1 = np.multiply((g - y_train.T) * g_der.T, W_2) / m\n",
    "        deriv_b2 = sum(g - y_train.T) / m\n",
    "\n",
    "        W_1 = W_1 - np.multiply(step_size, deriv_W1)\n",
    "        W_2 = W_2 - np.multiply(step_size, deriv_W2)\n",
    "\n",
    "        b_1 = b_1 - np.multiply(step_size, deriv_b1).T\n",
    "        b_2 = b_2 - np.multiply(step_size, deriv_b2)\n",
    "    return W_1, W_2, b_1, b_2\n",
    "\n",
    "# Cleaning up the data\n",
    "def classification_error(y_pred, y_true):\n",
    "    err = 1 - (np.sum(y_pred == y_true) / len(y_true))\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN ERROR: 0.380577\n"
     ]
    }
   ],
   "source": [
    "num_remove = 1\n",
    "data = []\n",
    "labels = []\n",
    "with open('wdbc.dat', \"r\") as data_file:\n",
    "    for line in data_file:\n",
    "        # get last char\n",
    "        line = line.strip()\n",
    "        # Converting the labels to 0 and 1 for binary classification\n",
    "        binary_label = 0 if line[-(num_remove):] is 'M' else 1\n",
    "        labels.append(binary_label)\n",
    "        line = line[:-(num_remove + 1)]\n",
    "        features = [float(x) for x in line.split(\",\")]\n",
    "        data.append(features)\n",
    "    c = list(zip(data, labels))\n",
    "    X, y = zip(*c)\n",
    "    y = np.matrix(y).T\n",
    "    X = np.matrix(X)\n",
    "\n",
    "# Normalization of the data --> raises the error?\n",
    "# mean = X.mean(axis=0)\n",
    "# std = X.std(axis=0)\n",
    "# X = (X - mean) / std\n",
    "\n",
    "# Parameters we can play around with\n",
    "step_size = 0.1\n",
    "iterations = 1000 \n",
    "func_type = \"sigmoid\" # can switch out with \"relu\" to see the results\n",
    "\n",
    "# Split the dataset into 2/3 training, 1/3 testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "m, d = X_train.shape\n",
    "\n",
    "# Initialize weight vectors W_1, W_2, b_1, b_2 for a single layer perceptron\n",
    "W_1 = np.random.rand(d1, d)\n",
    "W_2 = np.random.rand(1, d1)\n",
    "b_1 = np.zeros((d1, 1))\n",
    "b_2 = np.zeros(1)\n",
    "\n",
    "[W_1, W_2, b_1, b_2] = single_layer_perceptron(X_train, y_train, step_size, iterations, W_1, W_2, b_1, b_2, func_type)\n",
    "if (func_type == 'sigmoid'):\n",
    "    predicted_labels = np.sign(sigmoid(W_2 * sigmoid(W_1 * X_train.T + b_1) + b_2) - (1/2)).T;\n",
    "#     predicted_test_labels = np.sign(sigmoid(W_2 * sigmoid(W_1 * X_test.T + b_1) + b_2) - (1/2)).T;\n",
    "else:\n",
    "    predicted_labels = np.sign(sigmoid(W_2 * relu(W_1 * X_train.T + b_1) + b_2) - (1/2)).T;\n",
    "#     predicted_test_labels = np.sign(sigmoid(W_2 * relu(W_1 * X_test.T + b_1) + b_2) - (1/2)).T;\n",
    "    \n",
    "train_error = classification_error(predicted_labels, y_train);\n",
    "print(\"TRAIN ERROR: %f\" %(train_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
